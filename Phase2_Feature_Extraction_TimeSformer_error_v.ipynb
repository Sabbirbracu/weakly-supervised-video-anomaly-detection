{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "693e0958",
   "metadata": {},
   "source": [
    "# Phase 2: Feature Extraction with TimeSformer\n",
    "## Weakly Supervised Video Anomaly Detection using TimeSformer and MIL\n",
    "\n",
    "This notebook implements Phase 2: Extracting 768-dimensional features from video frames using a pretrained **TimeSformer** model.\n",
    "\n",
    "### Pipeline Overview:\n",
    "1. **GPU Setup & Verification** - Ensure CUDA is available and monitor GPU usage\n",
    "2. **Load Phase 1 Output** - Load extracted frames metadata\n",
    "3. **TimeSformer Model** - Load pretrained TimeSformer for feature extraction\n",
    "4. **Feature Extraction** - Extract [CLS] token features (768-dim) for each video\n",
    "5. **Save Features** - Store features for Phase 3 (MIL Training)\n",
    "\n",
    "### Key Concepts:\n",
    "- **TimeSformer**: Transformer-based video understanding model with divided space-time attention\n",
    "- **[CLS] Token**: 768-dimensional feature vector representing the entire video\n",
    "- **Batch Processing**: Process videos in batches for memory efficiency\n",
    "\n",
    "### Expected Input:\n",
    "- Extracted frames from Phase 1 (32 frames per video, 224√ó224 resolution)\n",
    "\n",
    "### Expected Output:\n",
    "- Feature vectors: (N_videos, 768) for each video\n",
    "- Saved as `.npy` files for efficient loading in Phase 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593ef1b9",
   "metadata": {},
   "source": [
    "## Section 1: Environment Setup & GPU Verification\n",
    "\n",
    "First, let's verify that PyTorch can access the GPU and set up monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "781605a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "GPU STATUS CHECK\n",
      "======================================================================\n",
      "\n",
      "üì¶ PyTorch Version: 2.7.1+cu118\n",
      "üîß CUDA Available: True\n",
      "\n",
      "üñ•Ô∏è  GPU Device: NVIDIA GeForce RTX 3080 Ti\n",
      "üìä Device Count: 1\n",
      "üî¢ CUDA Capability: (8, 6)\n",
      "\n",
      "üíæ Memory Status:\n",
      "   Total Memory: 12.0 GB\n",
      "   Allocated: 0.0 GB\n",
      "   Cached: 0.0 GB\n",
      "   Free: 12.0 GB\n",
      "\n",
      "‚ö° cuDNN Available: True\n",
      "   cuDNN Version: 90100\n",
      "\n",
      "======================================================================\n",
      "‚úÖ GPU IS READY FOR FEATURE EXTRACTION!\n",
      "======================================================================\n",
      "\n",
      "üéØ Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Phase 2: Feature Extraction with TimeSformer\n",
    "Environment Setup and GPU Verification\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import json\n",
    "import logging\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==================== GPU Verification ====================\n",
    "def check_gpu_status():\n",
    "    \"\"\"\n",
    "    Comprehensive GPU status check for PyTorch.\n",
    "    Returns detailed information about GPU availability and properties.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"GPU STATUS CHECK\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    gpu_info = {\n",
    "        'cuda_available': torch.cuda.is_available(),\n",
    "        'device_count': 0,\n",
    "        'current_device': None,\n",
    "        'device_name': None,\n",
    "        'device_capability': None,\n",
    "        'total_memory_gb': 0,\n",
    "        'allocated_memory_gb': 0,\n",
    "        'cached_memory_gb': 0,\n",
    "        'free_memory_gb': 0,\n",
    "        'cudnn_available': torch.backends.cudnn.is_available(),\n",
    "        'cudnn_version': None,\n",
    "        'pytorch_version': torch.__version__\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüì¶ PyTorch Version: {torch.__version__}\")\n",
    "    print(f\"üîß CUDA Available: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        gpu_info['device_count'] = torch.cuda.device_count()\n",
    "        gpu_info['current_device'] = torch.cuda.current_device()\n",
    "        gpu_info['device_name'] = torch.cuda.get_device_name(0)\n",
    "        gpu_info['device_capability'] = torch.cuda.get_device_capability(0)\n",
    "        \n",
    "        # Memory info\n",
    "        total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "        allocated_memory = torch.cuda.memory_allocated(0)\n",
    "        cached_memory = torch.cuda.memory_reserved(0)\n",
    "        \n",
    "        gpu_info['total_memory_gb'] = round(total_memory / (1024**3), 2)\n",
    "        gpu_info['allocated_memory_gb'] = round(allocated_memory / (1024**3), 2)\n",
    "        gpu_info['cached_memory_gb'] = round(cached_memory / (1024**3), 2)\n",
    "        gpu_info['free_memory_gb'] = round((total_memory - allocated_memory) / (1024**3), 2)\n",
    "        \n",
    "        if torch.backends.cudnn.is_available():\n",
    "            gpu_info['cudnn_version'] = torch.backends.cudnn.version()\n",
    "        \n",
    "        print(f\"\\nüñ•Ô∏è  GPU Device: {gpu_info['device_name']}\")\n",
    "        print(f\"üìä Device Count: {gpu_info['device_count']}\")\n",
    "        print(f\"üî¢ CUDA Capability: {gpu_info['device_capability']}\")\n",
    "        print(f\"\\nüíæ Memory Status:\")\n",
    "        print(f\"   Total Memory: {gpu_info['total_memory_gb']} GB\")\n",
    "        print(f\"   Allocated: {gpu_info['allocated_memory_gb']} GB\")\n",
    "        print(f\"   Cached: {gpu_info['cached_memory_gb']} GB\")\n",
    "        print(f\"   Free: {gpu_info['free_memory_gb']} GB\")\n",
    "        print(f\"\\n‚ö° cuDNN Available: {gpu_info['cudnn_available']}\")\n",
    "        if gpu_info['cudnn_version']:\n",
    "            print(f\"   cuDNN Version: {gpu_info['cudnn_version']}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"‚úÖ GPU IS READY FOR FEATURE EXTRACTION!\")\n",
    "        print(\"=\"*70)\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"‚ùå NO GPU AVAILABLE - Will use CPU (MUCH SLOWER!)\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\n‚ö†Ô∏è  Tips to enable GPU:\")\n",
    "        print(\"   1. Install CUDA Toolkit: https://developer.nvidia.com/cuda-downloads\")\n",
    "        print(\"   2. Install PyTorch with CUDA: pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\")\n",
    "        print(\"   3. Verify NVIDIA drivers are installed\")\n",
    "    \n",
    "    return gpu_info\n",
    "\n",
    "\n",
    "def get_gpu_memory_usage():\n",
    "    \"\"\"Get current GPU memory usage.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "        cached = torch.cuda.memory_reserved(0) / (1024**3)\n",
    "        return f\"Allocated: {allocated:.2f}GB, Cached: {cached:.2f}GB\"\n",
    "    return \"GPU not available\"\n",
    "\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU memory cache.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        print(f\"üßπ GPU memory cleared. Current usage: {get_gpu_memory_usage()}\")\n",
    "\n",
    "\n",
    "# Run GPU check\n",
    "gpu_info = check_gpu_status()\n",
    "\n",
    "# Set device\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nüéØ Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36977dd7",
   "metadata": {},
   "source": [
    "## Section 2: Configuration\n",
    "\n",
    "Set up paths and parameters for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ec234f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PHASE 2 CONFIGURATION\n",
      "======================================================================\n",
      "\n",
      "üìÅ Paths:\n",
      "   Extracted Frames: C:\\UCF_video_dataset\\Extracted_Frames\n",
      "   Features Output: C:\\UCF_video_dataset\\TimeSformer_Features\n",
      "\n",
      "ü§ñ Model:\n",
      "   TimeSformer: facebook/timesformer-base-finetuned-k400\n",
      "   Feature Dimension: 768\n",
      "\n",
      "üé¨ Video Parameters:\n",
      "   Frames per Video: 32\n",
      "   Frame Resolution: 224√ó224\n",
      "\n",
      "‚öôÔ∏è  Processing:\n",
      "   Batch Size: 1\n",
      "   Device: cuda\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Configuration for Phase 2: Feature Extraction\n",
    "\"\"\"\n",
    "\n",
    "# ==================== Dataset Paths ====================\n",
    "DATASET_ROOT = r\"C:\\UCF_video_dataset\"\n",
    "EXTRACTED_FRAMES_PATH = os.path.join(DATASET_ROOT, \"Extracted_Frames\")\n",
    "FEATURES_PATH = os.path.join(DATASET_ROOT, \"TimeSformer_Features\")\n",
    "\n",
    "# ==================== UCF-Crime Dataset Categories ====================\n",
    "ANOMALY_CATEGORIES = [\n",
    "    \"Abuse\", \"Arrest\", \"Arson\", \"Assault\", \"Burglary\",\n",
    "    \"Explosion\", \"Fighting\", \"RoadAccidents\", \"Robbery\",\n",
    "    \"Shooting\", \"Shoplifting\", \"Stealing\", \"Vandalism\"\n",
    "]\n",
    "NORMAL_CATEGORY = \"Normal\"\n",
    "ALL_CATEGORIES = ANOMALY_CATEGORIES + [NORMAL_CATEGORY]\n",
    "\n",
    "# ==================== TimeSformer Parameters ====================\n",
    "# Model configuration\n",
    "TIMESFORMER_MODEL = \"facebook/timesformer-base-finetuned-k400\"  # Pretrained on Kinetics-400\n",
    "FEATURE_DIM = 768  # TimeSformer [CLS] token dimension\n",
    "\n",
    "# Frame parameters (must match Phase 1)\n",
    "NUM_FRAMES_PER_VIDEO = 32\n",
    "FRAME_HEIGHT = 224\n",
    "FRAME_WIDTH = 224\n",
    "\n",
    "# ==================== Processing Parameters ====================\n",
    "BATCH_SIZE = 1  # Process one video at a time to avoid OOM\n",
    "NUM_WORKERS = 0  # DataLoader workers (0 for Windows compatibility)\n",
    "\n",
    "# ==================== Output Settings ====================\n",
    "SAVE_INDIVIDUAL_FEATURES = True  # Save features per video\n",
    "SAVE_COMBINED_FEATURES = True    # Save all features in one file\n",
    "\n",
    "# ==================== Logging ====================\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(FEATURES_PATH, exist_ok=True)\n",
    "\n",
    "# Print configuration\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PHASE 2 CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìÅ Paths:\")\n",
    "print(f\"   Extracted Frames: {EXTRACTED_FRAMES_PATH}\")\n",
    "print(f\"   Features Output: {FEATURES_PATH}\")\n",
    "print(f\"\\nü§ñ Model:\")\n",
    "print(f\"   TimeSformer: {TIMESFORMER_MODEL}\")\n",
    "print(f\"   Feature Dimension: {FEATURE_DIM}\")\n",
    "print(f\"\\nüé¨ Video Parameters:\")\n",
    "print(f\"   Frames per Video: {NUM_FRAMES_PER_VIDEO}\")\n",
    "print(f\"   Frame Resolution: {FRAME_HEIGHT}√ó{FRAME_WIDTH}\")\n",
    "print(f\"\\n‚öôÔ∏è  Processing:\")\n",
    "print(f\"   Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"   Device: {DEVICE}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb3fe27",
   "metadata": {},
   "source": [
    "## Section 3: Install Required Packages\n",
    "\n",
    "Install TimeSformer and related packages if not already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1f5d378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Pillow already installed\n",
      "‚úì OpenCV already installed\n",
      "\n",
      "üì¶ Installing: ['transformers']\n",
      "   Installing transformers...\n",
      "\n",
      "‚úì All packages installed successfully!\n",
      "‚ö†Ô∏è  Please restart the kernel and run again.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "# Run this cell only once\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "\n",
    "# Check and install required packages\n",
    "packages_to_install = []\n",
    "\n",
    "try:\n",
    "    from transformers import TimesformerModel, AutoImageProcessor\n",
    "    print(\"‚úì transformers (with TimeSformer) already installed\")\n",
    "except ImportError:\n",
    "    packages_to_install.append(\"transformers\")\n",
    "\n",
    "try:\n",
    "    from PIL import Image\n",
    "    print(\"‚úì Pillow already installed\")\n",
    "except ImportError:\n",
    "    packages_to_install.append(\"Pillow\")\n",
    "\n",
    "try:\n",
    "    import cv2\n",
    "    print(\"‚úì OpenCV already installed\")\n",
    "except ImportError:\n",
    "    packages_to_install.append(\"opencv-python\")\n",
    "\n",
    "if packages_to_install:\n",
    "    print(f\"\\nüì¶ Installing: {packages_to_install}\")\n",
    "    for pkg in packages_to_install:\n",
    "        print(f\"   Installing {pkg}...\")\n",
    "        install_package(pkg)\n",
    "    print(\"\\n‚úì All packages installed successfully!\")\n",
    "    print(\"‚ö†Ô∏è  Please restart the kernel and run again.\")\n",
    "else:\n",
    "    print(\"\\n‚úì All required packages are already installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7a60b0",
   "metadata": {},
   "source": [
    "## Section 4: Load TimeSformer Model\n",
    "\n",
    "Load the pretrained TimeSformer model and image processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "646a9751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Initializing TimeSformer Feature Extractor...\n",
      "\n",
      "======================================================================\n",
      "LOADING TIMESFORMER MODEL\n",
      "======================================================================\n",
      "\n",
      "üì• Downloading/Loading: facebook/timesformer-base-finetuned-k400\n",
      "   This may take a few minutes on first run...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3989653eab7a4bd59afb2ad14425cfbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/412 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Image processor loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca4633368fc4c47abaf69952d412a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "2026-01-20 17:47:17,081 - WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56cef23f9f4c46d7b30af4316c77ae84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/486M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Model loaded and moved to cuda\n",
      "\n",
      "üìä Model Statistics:\n",
      "   Total Parameters: 121,258,752\n",
      "   Trainable Parameters: 121,258,752\n",
      "   Model Size: ~0.45 GB (FP32)\n",
      "   Load Time: 23.19 seconds\n",
      "\n",
      "üíæ GPU Memory: Allocated: 0.45GB, Cached: 0.51GB\n",
      "\n",
      "======================================================================\n",
      "‚úÖ TIMESFORMER READY FOR FEATURE EXTRACTION!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load TimeSformer Model for Feature Extraction\n",
    "\"\"\"\n",
    "\n",
    "from transformers import TimesformerModel, AutoImageProcessor\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "class TimeSformerFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Feature extractor using pretrained TimeSformer model.\n",
    "    Extracts 768-dimensional [CLS] token features from video frames.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = TIMESFORMER_MODEL,\n",
    "        device: torch.device = DEVICE,\n",
    "        num_frames: int = NUM_FRAMES_PER_VIDEO\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the TimeSformer feature extractor.\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model identifier\n",
    "            device: torch device (cuda/cpu)\n",
    "            num_frames: Number of frames per video\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.num_frames = num_frames\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"LOADING TIMESFORMER MODEL\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"\\nüì• Downloading/Loading: {model_name}\")\n",
    "        print(f\"   This may take a few minutes on first run...\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Load image processor\n",
    "        self.image_processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "        print(f\"   ‚úì Image processor loaded\")\n",
    "        \n",
    "        # Load model\n",
    "        self.model = TimesformerModel.from_pretrained(model_name)\n",
    "        self.model = self.model.to(device)\n",
    "        self.model.eval()  # Set to evaluation mode\n",
    "        print(f\"   ‚úì Model loaded and moved to {device}\")\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        # Get model info\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"\\nüìä Model Statistics:\")\n",
    "        print(f\"   Total Parameters: {total_params:,}\")\n",
    "        print(f\"   Trainable Parameters: {trainable_params:,}\")\n",
    "        print(f\"   Model Size: ~{total_params * 4 / (1024**3):.2f} GB (FP32)\")\n",
    "        print(f\"   Load Time: {elapsed:.2f} seconds\")\n",
    "        \n",
    "        # Check GPU memory after loading\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"\\nüíæ GPU Memory: {get_gpu_memory_usage()}\")\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"‚úÖ TIMESFORMER READY FOR FEATURE EXTRACTION!\")\n",
    "        print(f\"{'='*70}\")\n",
    "    \n",
    "    def load_frames_from_directory(self, video_dir: str) -> Optional[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Load frames from a directory containing extracted frames.\n",
    "        \n",
    "        Args:\n",
    "            video_dir: Path to directory containing frame images\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of shape (1, num_frames, C, H, W) or None if loading fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get sorted list of frame files\n",
    "            frame_files = sorted([\n",
    "                f for f in os.listdir(video_dir) \n",
    "                if f.endswith(('.jpg', '.jpeg', '.png'))\n",
    "            ])\n",
    "            \n",
    "            if len(frame_files) < self.num_frames:\n",
    "                logger.warning(f\"Not enough frames in {video_dir}: {len(frame_files)} < {self.num_frames}\")\n",
    "                return None\n",
    "            \n",
    "            # Select frames (use first num_frames if more available)\n",
    "            selected_files = frame_files[:self.num_frames]\n",
    "            \n",
    "            # Load frames\n",
    "            frames = []\n",
    "            for frame_file in selected_files:\n",
    "                frame_path = os.path.join(video_dir, frame_file)\n",
    "                frame = Image.open(frame_path).convert('RGB')\n",
    "                frames.append(frame)\n",
    "            \n",
    "            # Process frames using the image processor\n",
    "            inputs = self.image_processor(frames, return_tensors=\"pt\")\n",
    "            \n",
    "            return inputs['pixel_values']  # Shape: (1, num_frames, C, H, W)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading frames from {video_dir}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def extract_features(self, pixel_values: torch.Tensor) -> Optional[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Extract features from video frames using TimeSformer.\n",
    "        \n",
    "        Args:\n",
    "            pixel_values: Tensor of shape (batch, num_frames, C, H, W)\n",
    "            \n",
    "        Returns:\n",
    "            Feature array of shape (batch, 768) - [CLS] token features\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Move to device\n",
    "            pixel_values = pixel_values.to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self.model(pixel_values)\n",
    "            \n",
    "            # Get [CLS] token features (first token of last hidden state)\n",
    "            # last_hidden_state shape: (batch, num_patches + 1, hidden_size)\n",
    "            cls_features = outputs.last_hidden_state[:, 0, :]  # Shape: (batch, 768)\n",
    "            \n",
    "            return cls_features.cpu().numpy()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting features: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def process_video(self, video_dir: str) -> Optional[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Process a single video directory and extract features.\n",
    "        \n",
    "        Args:\n",
    "            video_dir: Path to directory containing video frames\n",
    "            \n",
    "        Returns:\n",
    "            Feature array of shape (768,) or None if processing fails\n",
    "        \"\"\"\n",
    "        # Load frames\n",
    "        pixel_values = self.load_frames_from_directory(video_dir)\n",
    "        \n",
    "        if pixel_values is None:\n",
    "            return None\n",
    "        \n",
    "        # Extract features\n",
    "        features = self.extract_features(pixel_values)\n",
    "        \n",
    "        if features is None:\n",
    "            return None\n",
    "        \n",
    "        return features[0]  # Return first (and only) batch item\n",
    "\n",
    "\n",
    "# Initialize the feature extractor\n",
    "print(\"\\nüöÄ Initializing TimeSformer Feature Extractor...\")\n",
    "feature_extractor = TimeSformerFeatureExtractor(\n",
    "    model_name=TIMESFORMER_MODEL,\n",
    "    device=DEVICE,\n",
    "    num_frames=NUM_FRAMES_PER_VIDEO\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1a9919",
   "metadata": {},
   "source": [
    "## Section 5: GPU Monitoring During Extraction\n",
    "\n",
    "Utility functions to monitor GPU usage during feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f431d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ GPU Memory Status:\n",
      "   Allocated: 0.45 GB\n",
      "   Cached: 0.51 GB\n",
      "   Peak: 0.45 GB\n",
      "   Total: 12.00 GB\n",
      "   Utilization: 3.8%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "GPU Monitoring Utilities\n",
    "\"\"\"\n",
    "\n",
    "class GPUMonitor:\n",
    "    \"\"\"\n",
    "    Monitor GPU usage during processing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.is_available = torch.cuda.is_available()\n",
    "        self.measurements = []\n",
    "        \n",
    "    def measure(self):\n",
    "        \"\"\"Take a measurement of current GPU memory usage.\"\"\"\n",
    "        if self.is_available:\n",
    "            allocated = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "            cached = torch.cuda.memory_reserved(0) / (1024**3)\n",
    "            self.measurements.append({\n",
    "                'timestamp': time.time(),\n",
    "                'allocated_gb': allocated,\n",
    "                'cached_gb': cached\n",
    "            })\n",
    "            return allocated, cached\n",
    "        return 0, 0\n",
    "    \n",
    "    def get_peak_usage(self):\n",
    "        \"\"\"Get peak memory usage.\"\"\"\n",
    "        if self.is_available:\n",
    "            return torch.cuda.max_memory_allocated(0) / (1024**3)\n",
    "        return 0\n",
    "    \n",
    "    def reset_peak(self):\n",
    "        \"\"\"Reset peak memory statistics.\"\"\"\n",
    "        if self.is_available:\n",
    "            torch.cuda.reset_peak_memory_stats(0)\n",
    "    \n",
    "    def print_status(self):\n",
    "        \"\"\"Print current GPU status.\"\"\"\n",
    "        if self.is_available:\n",
    "            allocated, cached = self.measure()\n",
    "            peak = self.get_peak_usage()\n",
    "            total = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "            print(f\"\\nüíæ GPU Memory Status:\")\n",
    "            print(f\"   Allocated: {allocated:.2f} GB\")\n",
    "            print(f\"   Cached: {cached:.2f} GB\")\n",
    "            print(f\"   Peak: {peak:.2f} GB\")\n",
    "            print(f\"   Total: {total:.2f} GB\")\n",
    "            print(f\"   Utilization: {(allocated/total)*100:.1f}%\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è  GPU not available\")\n",
    "\n",
    "\n",
    "# Test GPU monitoring\n",
    "gpu_monitor = GPUMonitor()\n",
    "gpu_monitor.print_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943764eb",
   "metadata": {},
   "source": [
    "## Section 6: Load Phase 1 Metadata\n",
    "\n",
    "Load the extraction metadata from Phase 1 to get the list of videos to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "927cc4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "LOADING PHASE 1 METADATA\n",
      "======================================================================\n",
      "\n",
      "‚úì Loaded: C:\\UCF_video_dataset\\Extracted_Frames\\extraction_metadata.json\n",
      "\n",
      "üìä Dataset Summary:\n",
      "   Total Videos: 1900\n",
      "   Anomaly Videos: 950\n",
      "   Normal Videos: 950\n",
      "\n",
      "üìÅ Per-Category Counts:\n",
      "   Abuse                [ANOMALY ]:   50 videos\n",
      "   Arrest               [ANOMALY ]:   50 videos\n",
      "   Arson                [ANOMALY ]:   50 videos\n",
      "   Assault              [ANOMALY ]:   50 videos\n",
      "   Burglary             [ANOMALY ]:  100 videos\n",
      "   Explosion            [ANOMALY ]:   50 videos\n",
      "   Fighting             [ANOMALY ]:   50 videos\n",
      "   Normal               [NORMAL  ]:  950 videos\n",
      "   RoadAccidents        [ANOMALY ]:  150 videos\n",
      "   Robbery              [ANOMALY ]:  150 videos\n",
      "   Shooting             [ANOMALY ]:   50 videos\n",
      "   Shoplifting          [ANOMALY ]:   50 videos\n",
      "   Stealing             [ANOMALY ]:  100 videos\n",
      "   Vandalism            [ANOMALY ]:   50 videos\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load Phase 1 Extraction Metadata\n",
    "\"\"\"\n",
    "\n",
    "def load_phase1_metadata(extracted_frames_path: str = EXTRACTED_FRAMES_PATH) -> Dict:\n",
    "    \"\"\"\n",
    "    Load metadata from Phase 1 frame extraction.\n",
    "    \n",
    "    Args:\n",
    "        extracted_frames_path: Path to extracted frames directory\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing video metadata\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LOADING PHASE 1 METADATA\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Check for metadata files\n",
    "    metadata_path = os.path.join(extracted_frames_path, 'extraction_metadata.json')\n",
    "    alt_metadata_path = os.path.join(extracted_frames_path, 'dataset_metadata.json')\n",
    "    \n",
    "    metadata = None\n",
    "    \n",
    "    if os.path.exists(metadata_path):\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        print(f\"\\n‚úì Loaded: {metadata_path}\")\n",
    "    elif os.path.exists(alt_metadata_path):\n",
    "        with open(alt_metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        print(f\"\\n‚úì Loaded: {alt_metadata_path}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå No metadata file found!\")\n",
    "        print(f\"   Expected: {metadata_path}\")\n",
    "        print(f\"   Or: {alt_metadata_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Parse metadata\n",
    "    videos_info = []\n",
    "    \n",
    "    if 'videos' in metadata:\n",
    "        # GPU extraction format\n",
    "        for video in metadata['videos']:\n",
    "            if video.get('status') in ['success', 'skipped']:\n",
    "                videos_info.append({\n",
    "                    'video_name': video.get('video_name'),\n",
    "                    'category': video.get('category'),\n",
    "                    'is_anomaly': video.get('is_anomaly', video.get('category') != NORMAL_CATEGORY)\n",
    "                })\n",
    "    elif 'categories' in metadata:\n",
    "        # UCFCrimeDatasetProcessor format\n",
    "        for category, cat_info in metadata['categories'].items():\n",
    "            for video in cat_info.get('videos', []):\n",
    "                if video.get('status') == 'success':\n",
    "                    videos_info.append({\n",
    "                        'video_name': video.get('video_name'),\n",
    "                        'category': category,\n",
    "                        'is_anomaly': category != NORMAL_CATEGORY\n",
    "                    })\n",
    "    \n",
    "    # Count by category\n",
    "    category_counts = {}\n",
    "    anomaly_count = 0\n",
    "    normal_count = 0\n",
    "    \n",
    "    for video in videos_info:\n",
    "        cat = video['category']\n",
    "        category_counts[cat] = category_counts.get(cat, 0) + 1\n",
    "        if video['is_anomaly']:\n",
    "            anomaly_count += 1\n",
    "        else:\n",
    "            normal_count += 1\n",
    "    \n",
    "    print(f\"\\nüìä Dataset Summary:\")\n",
    "    print(f\"   Total Videos: {len(videos_info)}\")\n",
    "    print(f\"   Anomaly Videos: {anomaly_count}\")\n",
    "    print(f\"   Normal Videos: {normal_count}\")\n",
    "    \n",
    "    print(f\"\\nüìÅ Per-Category Counts:\")\n",
    "    for cat in sorted(category_counts.keys()):\n",
    "        label = \"ANOMALY\" if cat != NORMAL_CATEGORY else \"NORMAL\"\n",
    "        print(f\"   {cat:20s} [{label:8s}]: {category_counts[cat]:4d} videos\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return {\n",
    "        'videos': videos_info,\n",
    "        'total_videos': len(videos_info),\n",
    "        'anomaly_count': anomaly_count,\n",
    "        'normal_count': normal_count,\n",
    "        'category_counts': category_counts\n",
    "    }\n",
    "\n",
    "\n",
    "# Load metadata\n",
    "phase1_metadata = load_phase1_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4868344",
   "metadata": {},
   "source": [
    "## Section 7: Test Feature Extraction on Single Video\n",
    "\n",
    "Test the feature extraction pipeline on a single video before processing the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "348ade99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TESTING FEATURE EXTRACTION ON SINGLE VIDEO\n",
      "======================================================================\n",
      "\n",
      "üé¨ Test Video: Abuse001_x264\n",
      "   Category: Abuse\n",
      "   Path: C:\\UCF_video_dataset\\Extracted_Frames\\Abuse\\Abuse001_x264\n",
      "   Frames: 32\n",
      "\n",
      "‚è≥ Extracting features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "2026-01-20 17:47:39,647 - WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ab867598de44d7a95df508d1b6be77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/486M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Feature Extraction Successful!\n",
      "   Feature Shape: (768,)\n",
      "   Feature Dtype: float32\n",
      "   Feature Range: [-3.1207, 3.2564]\n",
      "   Feature Mean: -0.0141\n",
      "   Feature Std: 0.9689\n",
      "   Extraction Time: 3.82 seconds\n",
      "\n",
      "üíæ GPU Peak Memory: 1.10 GB\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test Feature Extraction on Single Video\n",
    "\"\"\"\n",
    "\n",
    "def test_single_video_extraction(feature_extractor, metadata):\n",
    "    \"\"\"\n",
    "    Test feature extraction on a single video.\n",
    "    \n",
    "    Args:\n",
    "        feature_extractor: TimeSformerFeatureExtractor instance\n",
    "        metadata: Phase 1 metadata dictionary\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TESTING FEATURE EXTRACTION ON SINGLE VIDEO\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if metadata is None or len(metadata['videos']) == 0:\n",
    "        print(\"\\n‚ùå No videos available for testing!\")\n",
    "        return None\n",
    "    \n",
    "    # Get first video\n",
    "    test_video = metadata['videos'][0]\n",
    "    video_name = test_video['video_name']\n",
    "    category = test_video['category']\n",
    "    \n",
    "    video_dir = os.path.join(EXTRACTED_FRAMES_PATH, category, video_name)\n",
    "    \n",
    "    print(f\"\\nüé¨ Test Video: {video_name}\")\n",
    "    print(f\"   Category: {category}\")\n",
    "    print(f\"   Path: {video_dir}\")\n",
    "    \n",
    "    # Check if directory exists\n",
    "    if not os.path.exists(video_dir):\n",
    "        print(f\"\\n‚ùå Video directory not found: {video_dir}\")\n",
    "        return None\n",
    "    \n",
    "    # Count frames\n",
    "    frame_count = len([f for f in os.listdir(video_dir) if f.endswith('.jpg')])\n",
    "    print(f\"   Frames: {frame_count}\")\n",
    "    \n",
    "    # Extract features\n",
    "    print(f\"\\n‚è≥ Extracting features...\")\n",
    "    \n",
    "    # Monitor GPU before\n",
    "    gpu_monitor.reset_peak()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    features = feature_extractor.process_video(video_dir)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    if features is not None:\n",
    "        print(f\"\\n‚úÖ Feature Extraction Successful!\")\n",
    "        print(f\"   Feature Shape: {features.shape}\")\n",
    "        print(f\"   Feature Dtype: {features.dtype}\")\n",
    "        print(f\"   Feature Range: [{features.min():.4f}, {features.max():.4f}]\")\n",
    "        print(f\"   Feature Mean: {features.mean():.4f}\")\n",
    "        print(f\"   Feature Std: {features.std():.4f}\")\n",
    "        print(f\"   Extraction Time: {elapsed:.2f} seconds\")\n",
    "        \n",
    "        # GPU usage\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"\\nüíæ GPU Peak Memory: {gpu_monitor.get_peak_usage():.2f} GB\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        return features\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Feature extraction failed!\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Run test\n",
    "test_features = test_single_video_extraction(feature_extractor, phase1_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7886a5df",
   "metadata": {},
   "source": [
    "## Section 8: Full Dataset Feature Extraction\n",
    "\n",
    "Extract features from all videos in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eebf161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Feature extraction function defined\n",
      "\n",
      "‚ö†Ô∏è  Run the next cell to start full dataset extraction.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Full Dataset Feature Extraction\n",
    "\"\"\"\n",
    "\n",
    "def extract_all_features(\n",
    "    feature_extractor: TimeSformerFeatureExtractor,\n",
    "    metadata: Dict,\n",
    "    extracted_frames_path: str = EXTRACTED_FRAMES_PATH,\n",
    "    output_path: str = FEATURES_PATH,\n",
    "    save_individual: bool = True,\n",
    "    resume: bool = True\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Extract features from all videos in the dataset.\n",
    "    \n",
    "    Args:\n",
    "        feature_extractor: TimeSformerFeatureExtractor instance\n",
    "        metadata: Phase 1 metadata dictionary\n",
    "        extracted_frames_path: Path to extracted frames\n",
    "        output_path: Path to save features\n",
    "        save_individual: Save individual video features\n",
    "        resume: Skip already processed videos\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing extraction results and all features\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FULL DATASET FEATURE EXTRACTION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if metadata is None:\n",
    "        print(\"\\n‚ùå No metadata available!\")\n",
    "        return None\n",
    "    \n",
    "    videos = metadata['videos']\n",
    "    total_videos = len(videos)\n",
    "    \n",
    "    print(f\"\\nüìä Videos to Process: {total_videos}\")\n",
    "    print(f\"   Resume Mode: {resume}\")\n",
    "    print(f\"   Save Individual: {save_individual}\")\n",
    "    print(f\"   Output Path: {output_path}\")\n",
    "    \n",
    "    # Create output directories\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    for category in ALL_CATEGORIES:\n",
    "        os.makedirs(os.path.join(output_path, category), exist_ok=True)\n",
    "    \n",
    "    # Estimate time\n",
    "    est_time_per_video = 2.0 if torch.cuda.is_available() else 10.0\n",
    "    est_total_time = total_videos * est_time_per_video\n",
    "    print(f\"\\n‚è±Ô∏è  Estimated Time: {est_total_time/60:.1f} minutes ({est_total_time/3600:.2f} hours)\")\n",
    "    \n",
    "    # Initialize tracking\n",
    "    results = {\n",
    "        'successful': 0,\n",
    "        'failed': 0,\n",
    "        'skipped': 0,\n",
    "        'videos': [],\n",
    "        'features': {},\n",
    "        'labels': {},\n",
    "        'processing_times': []\n",
    "    }\n",
    "    \n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    all_video_names = []\n",
    "    \n",
    "    # Reset GPU peak stats\n",
    "    gpu_monitor.reset_peak()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"\\nüöÄ Starting extraction...\\n\")\n",
    "    \n",
    "    # Process each video\n",
    "    pbar = tqdm(videos, desc=\"Extracting features\", unit=\"video\")\n",
    "    \n",
    "    for video_info in pbar:\n",
    "        video_name = video_info['video_name']\n",
    "        category = video_info['category']\n",
    "        is_anomaly = video_info['is_anomaly']\n",
    "        \n",
    "        video_dir = os.path.join(extracted_frames_path, category, video_name)\n",
    "        feature_path = os.path.join(output_path, category, f\"{video_name}.npy\")\n",
    "        \n",
    "        # Check if already processed\n",
    "        if resume and os.path.exists(feature_path):\n",
    "            try:\n",
    "                # Load existing features\n",
    "                features = np.load(feature_path)\n",
    "                if features.shape == (FEATURE_DIM,):\n",
    "                    results['skipped'] += 1\n",
    "                    all_features.append(features)\n",
    "                    all_labels.append(1 if is_anomaly else 0)\n",
    "                    all_video_names.append(video_name)\n",
    "                    pbar.set_postfix({'success': results['successful'], \n",
    "                                     'failed': results['failed'], \n",
    "                                     'skipped': results['skipped']})\n",
    "                    continue\n",
    "            except:\n",
    "                pass  # Re-process if loading fails\n",
    "        \n",
    "        # Check if video directory exists\n",
    "        if not os.path.exists(video_dir):\n",
    "            results['failed'] += 1\n",
    "            results['videos'].append({\n",
    "                'video_name': video_name,\n",
    "                'category': category,\n",
    "                'status': 'failed',\n",
    "                'error': 'Video directory not found'\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Extract features\n",
    "        video_start = time.time()\n",
    "        features = feature_extractor.process_video(video_dir)\n",
    "        video_elapsed = time.time() - video_start\n",
    "        \n",
    "        if features is not None:\n",
    "            results['successful'] += 1\n",
    "            results['processing_times'].append(video_elapsed)\n",
    "            \n",
    "            # Save individual feature file\n",
    "            if save_individual:\n",
    "                np.save(feature_path, features)\n",
    "            \n",
    "            # Store for combined output\n",
    "            all_features.append(features)\n",
    "            all_labels.append(1 if is_anomaly else 0)\n",
    "            all_video_names.append(video_name)\n",
    "            \n",
    "            results['videos'].append({\n",
    "                'video_name': video_name,\n",
    "                'category': category,\n",
    "                'is_anomaly': is_anomaly,\n",
    "                'feature_path': feature_path,\n",
    "                'status': 'success',\n",
    "                'processing_time': video_elapsed\n",
    "            })\n",
    "        else:\n",
    "            results['failed'] += 1\n",
    "            results['videos'].append({\n",
    "                'video_name': video_name,\n",
    "                'category': category,\n",
    "                'status': 'failed',\n",
    "                'error': 'Feature extraction failed'\n",
    "            })\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({'success': results['successful'], \n",
    "                         'failed': results['failed'], \n",
    "                         'skipped': results['skipped']})\n",
    "        \n",
    "        # Periodic GPU memory cleanup\n",
    "        if results['successful'] % 50 == 0 and torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    total_elapsed = time.time() - start_time\n",
    "    \n",
    "    # Save combined features\n",
    "    if len(all_features) > 0:\n",
    "        all_features_array = np.stack(all_features, axis=0)\n",
    "        all_labels_array = np.array(all_labels)\n",
    "        \n",
    "        # Save combined files\n",
    "        np.save(os.path.join(output_path, 'all_features.npy'), all_features_array)\n",
    "        np.save(os.path.join(output_path, 'all_labels.npy'), all_labels_array)\n",
    "        \n",
    "        # Save video names mapping\n",
    "        with open(os.path.join(output_path, 'video_names.json'), 'w') as f:\n",
    "            json.dump(all_video_names, f, indent=2)\n",
    "        \n",
    "        results['features_shape'] = all_features_array.shape\n",
    "        results['labels_shape'] = all_labels_array.shape\n",
    "    \n",
    "    # Save extraction metadata\n",
    "    extraction_metadata = {\n",
    "        'total_videos': total_videos,\n",
    "        'successful': results['successful'],\n",
    "        'failed': results['failed'],\n",
    "        'skipped': results['skipped'],\n",
    "        'feature_dim': FEATURE_DIM,\n",
    "        'model': TIMESFORMER_MODEL,\n",
    "        'processing_time_seconds': total_elapsed,\n",
    "        'processing_time_minutes': total_elapsed / 60,\n",
    "        'avg_time_per_video': np.mean(results['processing_times']) if results['processing_times'] else 0,\n",
    "        'device': str(DEVICE),\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'videos': results['videos']\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(output_path, 'extraction_metadata.json'), 'w') as f:\n",
    "        json.dump(extraction_metadata, f, indent=2)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FEATURE EXTRACTION COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nüìä Results:\")\n",
    "    print(f\"   Total Videos: {total_videos}\")\n",
    "    print(f\"   Successful: {results['successful']}\")\n",
    "    print(f\"   Failed: {results['failed']}\")\n",
    "    print(f\"   Skipped (resumed): {results['skipped']}\")\n",
    "    print(f\"\\n‚è±Ô∏è  Time:\")\n",
    "    print(f\"   Total: {total_elapsed/60:.2f} minutes\")\n",
    "    if results['processing_times']:\n",
    "        print(f\"   Average per video: {np.mean(results['processing_times']):.2f} seconds\")\n",
    "    print(f\"\\nüíæ Output:\")\n",
    "    print(f\"   Features Shape: {results.get('features_shape', 'N/A')}\")\n",
    "    print(f\"   Labels Shape: {results.get('labels_shape', 'N/A')}\")\n",
    "    print(f\"   Output Path: {output_path}\")\n",
    "    \n",
    "    # GPU stats\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"\\nüñ•Ô∏è  GPU Peak Memory: {gpu_monitor.get_peak_usage():.2f} GB\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"‚úì Feature extraction function defined\")\n",
    "print(\"\\n‚ö†Ô∏è  Run the next cell to start full dataset extraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41d9b54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FULL DATASET FEATURE EXTRACTION\n",
      "======================================================================\n",
      "\n",
      "üìä Videos to Process: 1900\n",
      "   Resume Mode: True\n",
      "   Save Individual: True\n",
      "   Output Path: C:\\UCF_video_dataset\\TimeSformer_Features\n",
      "\n",
      "‚è±Ô∏è  Estimated Time: 63.3 minutes (1.06 hours)\n",
      "\n",
      "üöÄ Starting extraction...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "147261ade64442cc9c644ca4adb6c83f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting features:   0%|          | 0/1900 [00:00<?, ?video/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FEATURE EXTRACTION COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "üìä Results:\n",
      "   Total Videos: 1900\n",
      "   Successful: 1900\n",
      "   Failed: 0\n",
      "   Skipped (resumed): 0\n",
      "\n",
      "‚è±Ô∏è  Time:\n",
      "   Total: 11.58 minutes\n",
      "   Average per video: 0.36 seconds\n",
      "\n",
      "üíæ Output:\n",
      "   Features Shape: (1900, 768)\n",
      "   Labels Shape: (1900,)\n",
      "   Output Path: C:\\UCF_video_dataset\\TimeSformer_Features\n",
      "\n",
      "üñ•Ô∏è  GPU Peak Memory: 1.10 GB\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run full dataset feature extraction\n",
    "# This will take approximately 1-2 hours depending on GPU\n",
    "\n",
    "extraction_results = extract_all_features(\n",
    "    feature_extractor=feature_extractor,\n",
    "    metadata=phase1_metadata,\n",
    "    extracted_frames_path=EXTRACTED_FRAMES_PATH,\n",
    "    output_path=FEATURES_PATH,\n",
    "    save_individual=True,  # Save individual .npy files per video\n",
    "    resume=True            # Skip already processed videos\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfbe0ef",
   "metadata": {},
   "source": [
    "## Section 9: Verify Extracted Features\n",
    "\n",
    "Verify that all features were extracted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04f7f566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FEATURE VERIFICATION REPORT\n",
      "======================================================================\n",
      "\n",
      "‚úì Combined Features File Found\n",
      "\n",
      "üìä Feature Statistics:\n",
      "   Shape: (1900, 768)\n",
      "   Dtype: float32\n",
      "   Min: -6.4888\n",
      "   Max: 6.1442\n",
      "   Mean: -0.0168\n",
      "   Std: 0.9737\n",
      "\n",
      "üìä Label Statistics:\n",
      "   Shape: (1900,)\n",
      "   Anomaly (1): 950\n",
      "   Normal (0): 950\n",
      "\n",
      "üìÅ Total Videos: 1900\n",
      "\n",
      "‚úì No NaN or Inf values detected\n",
      "\n",
      "üìÅ Per-Category Feature Files:\n",
      "   Abuse                [ANOMALY ]:   50 files\n",
      "   Arrest               [ANOMALY ]:   50 files\n",
      "   Arson                [ANOMALY ]:   50 files\n",
      "   Assault              [ANOMALY ]:   50 files\n",
      "   Burglary             [ANOMALY ]:  100 files\n",
      "   Explosion            [ANOMALY ]:   50 files\n",
      "   Fighting             [ANOMALY ]:   50 files\n",
      "   RoadAccidents        [ANOMALY ]:  150 files\n",
      "   Robbery              [ANOMALY ]:  150 files\n",
      "   Shooting             [ANOMALY ]:   50 files\n",
      "   Shoplifting          [ANOMALY ]:   50 files\n",
      "   Stealing             [ANOMALY ]:  100 files\n",
      "   Vandalism            [ANOMALY ]:   50 files\n",
      "   Normal               [NORMAL  ]:  950 files\n",
      "\n",
      "   Total Feature Files: 1900\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Verify Extracted Features\n",
    "\"\"\"\n",
    "\n",
    "def verify_features(features_path: str = FEATURES_PATH):\n",
    "    \"\"\"\n",
    "    Verify that all features were extracted correctly.\n",
    "    \n",
    "    Args:\n",
    "        features_path: Path to features directory\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FEATURE VERIFICATION REPORT\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Check combined files\n",
    "    combined_features_path = os.path.join(features_path, 'all_features.npy')\n",
    "    combined_labels_path = os.path.join(features_path, 'all_labels.npy')\n",
    "    video_names_path = os.path.join(features_path, 'video_names.json')\n",
    "    \n",
    "    if os.path.exists(combined_features_path):\n",
    "        features = np.load(combined_features_path)\n",
    "        labels = np.load(combined_labels_path)\n",
    "        \n",
    "        with open(video_names_path, 'r') as f:\n",
    "            video_names = json.load(f)\n",
    "        \n",
    "        print(f\"\\n‚úì Combined Features File Found\")\n",
    "        print(f\"\\nüìä Feature Statistics:\")\n",
    "        print(f\"   Shape: {features.shape}\")\n",
    "        print(f\"   Dtype: {features.dtype}\")\n",
    "        print(f\"   Min: {features.min():.4f}\")\n",
    "        print(f\"   Max: {features.max():.4f}\")\n",
    "        print(f\"   Mean: {features.mean():.4f}\")\n",
    "        print(f\"   Std: {features.std():.4f}\")\n",
    "        \n",
    "        print(f\"\\nüìä Label Statistics:\")\n",
    "        print(f\"   Shape: {labels.shape}\")\n",
    "        print(f\"   Anomaly (1): {np.sum(labels == 1)}\")\n",
    "        print(f\"   Normal (0): {np.sum(labels == 0)}\")\n",
    "        \n",
    "        print(f\"\\nüìÅ Total Videos: {len(video_names)}\")\n",
    "        \n",
    "        # Check for NaN or Inf\n",
    "        nan_count = np.sum(np.isnan(features))\n",
    "        inf_count = np.sum(np.isinf(features))\n",
    "        \n",
    "        if nan_count > 0 or inf_count > 0:\n",
    "            print(f\"\\n‚ö†Ô∏è  Data Issues:\")\n",
    "            print(f\"   NaN values: {nan_count}\")\n",
    "            print(f\"   Inf values: {inf_count}\")\n",
    "        else:\n",
    "            print(f\"\\n‚úì No NaN or Inf values detected\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Combined features file not found: {combined_features_path}\")\n",
    "    \n",
    "    # Count individual feature files\n",
    "    print(f\"\\nüìÅ Per-Category Feature Files:\")\n",
    "    total_files = 0\n",
    "    for category in ALL_CATEGORIES:\n",
    "        category_path = os.path.join(features_path, category)\n",
    "        if os.path.exists(category_path):\n",
    "            files = [f for f in os.listdir(category_path) if f.endswith('.npy')]\n",
    "            total_files += len(files)\n",
    "            label = \"ANOMALY\" if category != NORMAL_CATEGORY else \"NORMAL\"\n",
    "            print(f\"   {category:20s} [{label:8s}]: {len(files):4d} files\")\n",
    "    \n",
    "    print(f\"\\n   Total Feature Files: {total_files}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "\n",
    "# Run verification\n",
    "verify_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620475ba",
   "metadata": {},
   "source": [
    "## Section 10: Summary and Next Steps\n",
    "\n",
    "Summary of Phase 2 and preparation for Phase 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ee768cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PHASE 2 COMPLETE: FEATURE EXTRACTION\n",
      "======================================================================\n",
      "\n",
      "‚úÖ What was accomplished:\n",
      "   1. Loaded pretrained TimeSformer model (facebook/timesformer-base-finetuned-k400)\n",
      "   2. Extracted 768-dimensional [CLS] token features from each video\n",
      "   3. Saved features as individual .npy files and combined arrays\n",
      "   4. Generated metadata for tracking\n",
      "\n",
      "üìÅ Output Structure:\n",
      "   TimeSformer_Features/\n",
      "   ‚îú‚îÄ‚îÄ Abuse/\n",
      "   ‚îÇ   ‚îú‚îÄ‚îÄ video_001.npy  (768-dim feature vector)\n",
      "   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
      "   ‚îú‚îÄ‚îÄ Normal/\n",
      "   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
      "   ‚îú‚îÄ‚îÄ all_features.npy   (N_videos √ó 768)\n",
      "   ‚îú‚îÄ‚îÄ all_labels.npy     (N_videos,) - 1=anomaly, 0=normal\n",
      "   ‚îú‚îÄ‚îÄ video_names.json   (List of video names)\n",
      "   ‚îî‚îÄ‚îÄ extraction_metadata.json\n",
      "\n",
      "üöÄ Next Steps (Phase 3):\n",
      "   1. Load extracted features\n",
      "   2. Implement MIL (Multiple Instance Learning) network\n",
      "   3. Train with:\n",
      "      - Ranking Loss (anomaly > normal scores)\n",
      "      - Focal Loss (handle class imbalance)\n",
      "      - Temporal Smoothness Loss (consistent predictions)\n",
      "   4. Evaluate on test set\n",
      "\n",
      "======================================================================\n",
      "\n",
      "üíæ GPU Memory Status:\n",
      "   Allocated: 0.46 GB\n",
      "   Cached: 0.51 GB\n",
      "   Peak: 1.10 GB\n",
      "   Total: 12.00 GB\n",
      "   Utilization: 3.8%\n",
      "üßπ GPU memory cleared. Current usage: Allocated: 0.46GB, Cached: 0.51GB\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PHASE 2 COMPLETE: FEATURE EXTRACTION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "‚úÖ What was accomplished:\n",
    "   1. Loaded pretrained TimeSformer model (facebook/timesformer-base-finetuned-k400)\n",
    "   2. Extracted 768-dimensional [CLS] token features from each video\n",
    "   3. Saved features as individual .npy files and combined arrays\n",
    "   4. Generated metadata for tracking\n",
    "\n",
    "üìÅ Output Structure:\n",
    "   TimeSformer_Features/\n",
    "   ‚îú‚îÄ‚îÄ Abuse/\n",
    "   ‚îÇ   ‚îú‚îÄ‚îÄ video_001.npy  (768-dim feature vector)\n",
    "   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "   ‚îú‚îÄ‚îÄ Normal/\n",
    "   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "   ‚îú‚îÄ‚îÄ all_features.npy   (N_videos √ó 768)\n",
    "   ‚îú‚îÄ‚îÄ all_labels.npy     (N_videos,) - 1=anomaly, 0=normal\n",
    "   ‚îú‚îÄ‚îÄ video_names.json   (List of video names)\n",
    "   ‚îî‚îÄ‚îÄ extraction_metadata.json\n",
    "\n",
    "üöÄ Next Steps (Phase 3):\n",
    "   1. Load extracted features\n",
    "   2. Implement MIL (Multiple Instance Learning) network\n",
    "   3. Train with:\n",
    "      - Ranking Loss (anomaly > normal scores)\n",
    "      - Focal Loss (handle class imbalance)\n",
    "      - Temporal Smoothness Loss (consistent predictions)\n",
    "   4. Evaluate on test set\n",
    "\"\"\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Final GPU status\n",
    "if torch.cuda.is_available():\n",
    "    gpu_monitor.print_status()\n",
    "    clear_gpu_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
