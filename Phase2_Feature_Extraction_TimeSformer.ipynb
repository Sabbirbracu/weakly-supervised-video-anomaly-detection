{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "693e0958",
   "metadata": {},
   "source": [
    "# Phase 2: Feature Extraction with TimeSformer (MIL-Ready)\n",
    "## Weakly Supervised Video Anomaly Detection using TimeSformer and MIL\n",
    "\n",
    "This notebook implements **Phase 2**: Extracting 768-dimensional features from video **clips** using a pretrained **TimeSformer** model.\n",
    "\n",
    "### üîÑ Critical Update: \"Bag of Instances\" Architecture\n",
    "This version is aligned with the **Sliding Window + Dilated Sampling** approach from Phase 1:\n",
    "- **Input**: Each video has **50-200 clip subfolders** (clip_0000, clip_0001, ...)\n",
    "- **Each Clip**: Contains **16 frames** spanning ~2.5 seconds of video\n",
    "- **Output**: Feature matrix of shape `(Num_Clips, 768)` per video - the **\"Bag\"** for MIL\n",
    "\n",
    "### Pipeline Overview:\n",
    "1. **GPU Setup & Verification** - Ensure CUDA is available\n",
    "2. **Load Phase 1 Metadata** - Read `dataset_metadata.json`\n",
    "3. **TimeSformer Model** - Load pretrained model (frozen weights)\n",
    "4. **Batch Feature Extraction** - Process clips in batches (GPU-efficient)\n",
    "5. **Save Feature Bags** - Store `(N_clips, 768)` arrays for Phase 3 (MIL Training)\n",
    "\n",
    "### Expected Input (from Phase 1):\n",
    "```\n",
    "Processed_Clips/\n",
    "‚îú‚îÄ‚îÄ Explosion/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Explosion001/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ clip_0000/  (16 frames: img_000.jpg ... img_015.jpg)\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ clip_0001/\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ... (50-200 clips)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Explosion002/\n",
    "‚îú‚îÄ‚îÄ Fighting/\n",
    "‚îî‚îÄ‚îÄ Normal/\n",
    "```\n",
    "\n",
    "### Expected Output:\n",
    "```\n",
    "TimeSformer_Features/\n",
    "‚îú‚îÄ‚îÄ Explosion/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Explosion001.npy  ‚Üí Shape: (num_clips, 768)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Explosion002.npy\n",
    "‚îú‚îÄ‚îÄ Fighting/\n",
    "‚îî‚îÄ‚îÄ Normal/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593ef1b9",
   "metadata": {},
   "source": [
    "## Cell 1: Imports & Configuration\n",
    "Sets up paths, model parameters, and verifies GPU availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "781605a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "GPU STATUS CHECK\n",
      "======================================================================\n",
      "\n",
      "üì¶ PyTorch Version: 2.7.1+cu118\n",
      "üîß CUDA Available: True\n",
      "\n",
      "üñ•Ô∏è  GPU Device: NVIDIA GeForce RTX 3080 Ti\n",
      "üíæ Total Memory: 12.00 GB\n",
      "\n",
      "‚úÖ GPU IS READY FOR FEATURE EXTRACTION!\n",
      "======================================================================\n",
      "\n",
      "üéØ Using device: cuda\n",
      "\n",
      "üìÅ Configuration:\n",
      "   Metadata: C:\\UCF_video_dataset\\Processed_Clips\\dataset_metadata.json\n",
      "   Output: C:\\UCF_video_dataset\\TimeSformer_Features\n",
      "   Model: facebook/timesformer-base-finetuned-k400\n",
      "   Frames/Clip: 16\n",
      "   Batch Size: 8\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Phase 2: Feature Extraction with TimeSformer (MIL-Ready)\n",
    "Cell 1: Imports & Configuration\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoImageProcessor, TimesformerModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "# Input: Metadata file from Phase 1 (CRITICAL: Updated path)\n",
    "METADATA_PATH = r\"C:\\UCF_video_dataset\\Processed_Clips\\dataset_metadata.json\"\n",
    "\n",
    "# Output: Where to save the feature BAGS (.npy files)\n",
    "FEATURE_OUTPUT_DIR = r\"C:\\UCF_video_dataset\\TimeSformer_Features\"\n",
    "\n",
    "# Model Settings\n",
    "MODEL_CKPT = \"facebook/timesformer-base-finetuned-k400\"\n",
    "FEATURE_DIM = 768  # TimeSformer [CLS] token dimension\n",
    "\n",
    "# Frame parameters (MUST match Phase 1)\n",
    "NUM_FRAMES_PER_CLIP = 16  # Changed from 32 to 16 (aligned with Phase 1)\n",
    "\n",
    "# Processing Settings (Adjust BATCH_SIZE based on your GPU VRAM)\n",
    "BATCH_SIZE = 8  # Safe for RTX 3080 Ti (12GB). Try 16 if stable.\n",
    "NUM_WORKERS = 0  # Use 0 for Windows stability, 4 for Linux\n",
    "\n",
    "# ================= SYSTEM CHECK =================\n",
    "def check_gpu_status():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"GPU STATUS CHECK\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    print(f\"\\nüì¶ PyTorch Version: {torch.__version__}\")\n",
    "    print(f\"üîß CUDA Available: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "        \n",
    "        print(f\"\\nüñ•Ô∏è  GPU Device: {gpu_name}\")\n",
    "        print(f\"üíæ Total Memory: {total_memory:.2f} GB\")\n",
    "        print(f\"\\n‚úÖ GPU IS READY FOR FEATURE EXTRACTION!\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå NO GPU AVAILABLE - Will use CPU (MUCH SLOWER!)\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    return device\n",
    "\n",
    "# Run GPU check\n",
    "DEVICE = check_gpu_status()\n",
    "\n",
    "# Print configuration summary\n",
    "print(f\"\\nüéØ Using device: {DEVICE}\")\n",
    "print(f\"\\nüìÅ Configuration:\")\n",
    "print(f\"   Metadata: {METADATA_PATH}\")\n",
    "print(f\"   Output: {FEATURE_OUTPUT_DIR}\")\n",
    "print(f\"   Model: {MODEL_CKPT}\")\n",
    "print(f\"   Frames/Clip: {NUM_FRAMES_PER_CLIP}\")\n",
    "print(f\"   Batch Size: {BATCH_SIZE}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36977dd7",
   "metadata": {},
   "source": [
    "## Cell 2: The Dataset Class (Handling \"Bag of Clips\" Logic)\n",
    "\n",
    "This is the **engine**. It opens a specific video folder, finds all clip sub-folders, and prepares them for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ec234f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ VideoClipsDataset class defined.\n",
      "   This class handles the 'Bag of Clips' structure from Phase 1.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cell 2: VideoClipsDataset - Handles the \"Bag of Clips\" Structure\n",
    "\"\"\"\n",
    "\n",
    "class VideoClipsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Reads a single video's folder and returns all its clips as a batch.\n",
    "    Each video folder contains: clip_0000/, clip_0001/, ... clip_NNNN/\n",
    "    Each clip folder contains: img_000.jpg, img_001.jpg, ... img_015.jpg (16 frames)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, clips_root_path, processor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            clips_root_path: Path to video folder (e.g., .../Explosion/Explosion001/)\n",
    "            processor: HuggingFace AutoImageProcessor for TimeSformer\n",
    "        \"\"\"\n",
    "        self.clips_root = clips_root_path\n",
    "        self.processor = processor\n",
    "        \n",
    "        # Find all valid clip folders (clip_0000, clip_0001, ...)\n",
    "        self.clip_folders = sorted([\n",
    "            d for d in os.listdir(clips_root_path) \n",
    "            if os.path.isdir(os.path.join(clips_root_path, d)) and d.startswith(\"clip_\")\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.clip_folders)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Load a single clip (16 frames) and preprocess for TimeSformer.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor of shape (3, 16, 224, 224) - ready for TimeSformer\n",
    "        \"\"\"\n",
    "        clip_name = self.clip_folders[idx]\n",
    "        clip_path = os.path.join(self.clips_root, clip_name)\n",
    "        \n",
    "        # Load the 16 images for this clip\n",
    "        images = []\n",
    "        # Sort ensures temporal order (img_000.jpg, img_001.jpg, ...)\n",
    "        filenames = sorted([f for f in os.listdir(clip_path) if f.endswith(\".jpg\")])\n",
    "        \n",
    "        for fname in filenames:\n",
    "            img_path = os.path.join(clip_path, fname)\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            images.append(img)\n",
    "        \n",
    "        # Validation: Must have exactly 16 frames\n",
    "        if len(images) != NUM_FRAMES_PER_CLIP:\n",
    "            print(f\"‚ö†Ô∏è Warning: {clip_name} has {len(images)} frames. Expected {NUM_FRAMES_PER_CLIP}.\")\n",
    "            # Return zeros as placeholder (will be filtered out)\n",
    "            return torch.zeros((3, NUM_FRAMES_PER_CLIP, 224, 224))\n",
    "        \n",
    "        # Preprocess using HuggingFace Processor\n",
    "        # TimeSformer expects list of PIL images\n",
    "        inputs = self.processor(images=images, return_tensors=\"pt\")\n",
    "        \n",
    "        # Shape: [1, 3, 16, 224, 224] -> Remove batch dim -> [3, 16, 224, 224]\n",
    "        return inputs['pixel_values'].squeeze(0)\n",
    "\n",
    "\n",
    "print(\"‚úÖ VideoClipsDataset class defined.\")\n",
    "print(\"   This class handles the 'Bag of Clips' structure from Phase 1.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb3fe27",
   "metadata": {},
   "source": [
    "## Cell 3: Load TimeSformer Model\n",
    "\n",
    "Loads the pretrained TimeSformer from Facebook and **freezes weights** (we're only extracting features, not training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1f5d378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING TIMESFORMER MODEL\n",
      "======================================================================\n",
      "\n",
      "‚è≥ Downloading/Loading: facebook/timesformer-base-finetuned-k400\n",
      "   This may take a few minutes on first run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Image processor loaded\n",
      "   ‚úì Model loaded\n",
      "   ‚úì Model moved to cuda\n",
      "   ‚úì Weights frozen (no training, only feature extraction)\n",
      "\n",
      "üìä Model Statistics:\n",
      "   Total Parameters: 121,258,752\n",
      "   Model Size: ~0.45 GB (FP32)\n",
      "   Load Time: 2.34 seconds\n",
      "\n",
      "üíæ GPU Memory Used: 0.45 GB\n",
      "\n",
      "======================================================================\n",
      "‚úÖ TIMESFORMER READY FOR FEATURE EXTRACTION!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cell 3: Load TimeSformer Model\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING TIMESFORMER MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n‚è≥ Downloading/Loading: {MODEL_CKPT}\")\n",
    "print(\"   This may take a few minutes on first run...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load image processor (handles normalization, resizing)\n",
    "processor = AutoImageProcessor.from_pretrained(MODEL_CKPT)\n",
    "print(\"   ‚úì Image processor loaded\")\n",
    "\n",
    "# Load model\n",
    "model = TimesformerModel.from_pretrained(MODEL_CKPT)\n",
    "print(\"   ‚úì Model loaded\")\n",
    "\n",
    "# Move to GPU and set to evaluation mode\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "print(f\"   ‚úì Model moved to {DEVICE}\")\n",
    "\n",
    "# CRITICAL: Freeze all weights (no gradient computation)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "print(\"   ‚úì Weights frozen (no training, only feature extraction)\")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "# Print model info\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nüìä Model Statistics:\")\n",
    "print(f\"   Total Parameters: {total_params:,}\")\n",
    "print(f\"   Model Size: ~{total_params * 4 / (1024**3):.2f} GB (FP32)\")\n",
    "print(f\"   Load Time: {elapsed:.2f} seconds\")\n",
    "\n",
    "# GPU memory check\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "    print(f\"\\nüíæ GPU Memory Used: {allocated:.2f} GB\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"‚úÖ TIMESFORMER READY FOR FEATURE EXTRACTION!\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7a60b0",
   "metadata": {},
   "source": [
    "## Cell 4: Main Feature Extraction Loop\n",
    "\n",
    "Reads the `dataset_metadata.json` from Phase 1 and processes every video.  \n",
    "For each video, it creates a **feature bag** of shape `(num_clips, 768)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "646a9751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ extract_features_optimized() function defined.\n",
      "\n",
      "‚ö° Key optimization: Videos with >500 clips will be uniformly sampled.\n",
      "   This prevents huge videos (15000+ clips) from hanging the process.\n",
      "\n",
      "‚ö†Ô∏è  Run the next cell to START (will skip already-processed videos).\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cell 4: Main Feature Extraction Loop - OPTIMIZED with Clip Limit\n",
    "Key fixes:\n",
    "1. MAX_CLIPS_PER_VIDEO - limits huge videos (some have 15000+ clips!)\n",
    "2. Progress bar within video - see progress on large videos\n",
    "3. Better timeout handling\n",
    "\"\"\"\n",
    "\n",
    "# ===== CRITICAL: Clip limit for huge videos =====\n",
    "MAX_CLIPS_PER_VIDEO = 500  # Limit clips per video (500 is plenty for MIL)\n",
    "SHOW_PROGRESS_THRESHOLD = 50  # Show inner progress bar if video has > 50 clips\n",
    "\n",
    "def extract_features_optimized():\n",
    "    \"\"\"\n",
    "    OPTIMIZED extraction with clip limiting for huge videos.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ========== 1. LOAD METADATA ==========\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PHASE 2: FEATURE EXTRACTION (OPTIMIZED)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if not os.path.exists(METADATA_PATH):\n",
    "        print(f\"‚ùå Error: Metadata file not found!\")\n",
    "        print(f\"   Expected: {METADATA_PATH}\")\n",
    "        return\n",
    "\n",
    "    with open(METADATA_PATH, 'r') as f:\n",
    "        video_list = json.load(f)\n",
    "        \n",
    "    print(f\"\\nüìÇ Found {len(video_list)} videos to process.\")\n",
    "    print(f\"‚ö° Max clips per video: {MAX_CLIPS_PER_VIDEO} (limits huge videos)\")\n",
    "    \n",
    "    # Count already processed\n",
    "    already_done = 0\n",
    "    for vm in video_list:\n",
    "        save_path = os.path.join(FEATURE_OUTPUT_DIR, vm['class_name'], f\"{vm['video_name']}.npy\")\n",
    "        if os.path.exists(save_path):\n",
    "            already_done += 1\n",
    "    print(f\"‚úÖ Already processed: {already_done} videos (will be skipped)\")\n",
    "    print(f\"üìã Remaining: {len(video_list) - already_done} videos\")\n",
    "    \n",
    "    os.makedirs(FEATURE_OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    # ========== 2. TRACKING ==========\n",
    "    results = {\n",
    "        'successful': 0,\n",
    "        'failed': 0,\n",
    "        'skipped': 0,\n",
    "        'clipped': 0,  # Videos that were clip-limited\n",
    "        'total_clips_processed': 0,\n",
    "        'videos': []\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # ========== 3. PROCESS VIDEOS ==========\n",
    "    print(\"\\nüöÄ Starting extraction...\\n\")\n",
    "    \n",
    "    pbar = tqdm(video_list, desc=\"Processing Videos\", unit=\"vid\")\n",
    "    \n",
    "    for video_meta in pbar:\n",
    "        video_name = video_meta['video_name']\n",
    "        class_name = video_meta['class_name']\n",
    "        clips_path = video_meta['clips_path']\n",
    "        num_clips_expected = video_meta['num_clips']\n",
    "        \n",
    "        # Update progress bar description\n",
    "        pbar.set_postfix({\n",
    "            'video': video_name[:20],\n",
    "            'clips': num_clips_expected,\n",
    "            'done': results['successful']\n",
    "        })\n",
    "        \n",
    "        # Setup Output Path\n",
    "        save_dir = os.path.join(FEATURE_OUTPUT_DIR, class_name)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        save_path = os.path.join(save_dir, f\"{video_name}.npy\")\n",
    "        \n",
    "        # Skip if already processed\n",
    "        if os.path.exists(save_path):\n",
    "            results['skipped'] += 1\n",
    "            continue\n",
    "        \n",
    "        # Check if clips directory exists\n",
    "        if not os.path.exists(clips_path):\n",
    "            results['failed'] += 1\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # ========== 4. CREATE DATASET ==========\n",
    "            dataset = VideoClipsDataset(clips_path, processor)\n",
    "            original_len = len(dataset)\n",
    "            \n",
    "            if original_len == 0:\n",
    "                results['failed'] += 1\n",
    "                continue\n",
    "            \n",
    "            # ===== CLIP LIMITING =====\n",
    "            was_clipped = False\n",
    "            if original_len > MAX_CLIPS_PER_VIDEO:\n",
    "                # Uniformly sample MAX_CLIPS_PER_VIDEO clips\n",
    "                indices = np.linspace(0, original_len - 1, MAX_CLIPS_PER_VIDEO, dtype=int)\n",
    "                dataset.clip_folders = [dataset.clip_folders[i] for i in indices]\n",
    "                # Note: Only clip_folders needs updating (clip_files doesn't exist in this version)\n",
    "                was_clipped = True\n",
    "                results['clipped'] += 1\n",
    "                tqdm.write(f\"   ‚ö° {video_name}: {original_len} clips ‚Üí {MAX_CLIPS_PER_VIDEO} (sampled)\")\n",
    "\n",
    "            # DataLoader\n",
    "            loader = DataLoader(\n",
    "                dataset, \n",
    "                batch_size=BATCH_SIZE, \n",
    "                num_workers=NUM_WORKERS,\n",
    "                pin_memory=True if DEVICE.type == 'cuda' else False\n",
    "            )\n",
    "            \n",
    "            video_features = []\n",
    "            \n",
    "            # ========== 5. EXTRACT FEATURES ==========\n",
    "            with torch.no_grad():\n",
    "                # Show inner progress for large videos\n",
    "                if len(dataset) > SHOW_PROGRESS_THRESHOLD:\n",
    "                    batch_iter = tqdm(loader, desc=f\"  {video_name[:25]}\", leave=False, unit=\"batch\")\n",
    "                else:\n",
    "                    batch_iter = loader\n",
    "                    \n",
    "                for batch in batch_iter:\n",
    "                    batch = batch.to(DEVICE)\n",
    "                    outputs = model(pixel_values=batch)\n",
    "                    cls_features = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "                    video_features.append(cls_features)\n",
    "            \n",
    "            # ========== 6. SAVE ==========\n",
    "            full_video_features = np.concatenate(video_features, axis=0)\n",
    "            np.save(save_path, full_video_features)\n",
    "            \n",
    "            results['successful'] += 1\n",
    "            results['total_clips_processed'] += len(dataset)\n",
    "            results['videos'].append({\n",
    "                'video_name': video_name,\n",
    "                'class_name': class_name,\n",
    "                'original_clips': original_len,\n",
    "                'processed_clips': len(dataset),\n",
    "                'was_clipped': was_clipped,\n",
    "                'feature_shape': list(full_video_features.shape),\n",
    "                'feature_path': save_path\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"‚ùå Error {video_name}: {e}\")\n",
    "            results['failed'] += 1\n",
    "            \n",
    "        # Periodic cleanup\n",
    "        if results['successful'] % 50 == 0 and torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    pbar.close()\n",
    "    \n",
    "    # ========== 7. FINAL REPORT ==========\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ PHASE 2 COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nüìä Results:\")\n",
    "    print(f\"   Successful: {results['successful']}\")\n",
    "    print(f\"   Failed: {results['failed']}\")\n",
    "    print(f\"   Skipped (already done): {results['skipped']}\")\n",
    "    print(f\"   Clip-limited (huge videos): {results['clipped']}\")\n",
    "    print(f\"   Total Clips Processed: {results['total_clips_processed']}\")\n",
    "    print(f\"\\n‚è±Ô∏è  Time: {total_time/60:.2f} minutes ({total_time:.0f} seconds)\")\n",
    "    if results['successful'] > 0:\n",
    "        print(f\"   Avg per video: {total_time/results['successful']:.1f} seconds\")\n",
    "    print(f\"\\nüíæ Features saved to: {FEATURE_OUTPUT_DIR}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Save metadata\n",
    "    extraction_meta = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'total_videos': len(video_list),\n",
    "        'successful': results['successful'],\n",
    "        'failed': results['failed'],\n",
    "        'skipped': results['skipped'],\n",
    "        'clipped': results['clipped'],\n",
    "        'total_clips': results['total_clips_processed'],\n",
    "        'max_clips_per_video': MAX_CLIPS_PER_VIDEO,\n",
    "        'model': MODEL_CKPT,\n",
    "        'feature_dim': FEATURE_DIM,\n",
    "        'processing_time_seconds': total_time,\n",
    "        'videos': results['videos']\n",
    "    }\n",
    "    \n",
    "    meta_path = os.path.join(FEATURE_OUTPUT_DIR, 'extraction_metadata.json')\n",
    "    with open(meta_path, 'w') as f:\n",
    "        json.dump(extraction_meta, f, indent=2, default=str)\n",
    "    print(f\"\\nüìÑ Metadata saved to: {meta_path}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"‚úÖ extract_features_optimized() function defined.\")\n",
    "print(f\"\\n‚ö° Key optimization: Videos with >500 clips will be uniformly sampled.\")\n",
    "print(\"   This prevents huge videos (15000+ clips) from hanging the process.\")\n",
    "print(\"\\n‚ö†Ô∏è  Run the next cell to START (will skip already-processed videos).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1a9919",
   "metadata": {},
   "source": [
    "## Cell 5: Run Feature Extraction\n",
    "\n",
    "**‚ö†Ô∏è This will process all videos. Estimated time: 1-3 hours depending on dataset size.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f431d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PHASE 2: FEATURE EXTRACTION (OPTIMIZED)\n",
      "======================================================================\n",
      "\n",
      "üìÇ Found 1900 videos to process.\n",
      "‚ö° Max clips per video: 500 (limits huge videos)\n",
      "‚úÖ Already processed: 1900 videos (will be skipped)\n",
      "üìã Remaining: 0 videos\n",
      "\n",
      "üöÄ Starting extraction...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7632eeb9c4444019bbfe12262c65bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Videos:   0%|          | 0/1900 [00:00<?, ?vid/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "‚úÖ PHASE 2 COMPLETE\n",
      "======================================================================\n",
      "\n",
      "üìä Results:\n",
      "   Successful: 0\n",
      "   Failed: 0\n",
      "   Skipped (already done): 1900\n",
      "   Clip-limited (huge videos): 0\n",
      "   Total Clips Processed: 0\n",
      "\n",
      "‚è±Ô∏è  Time: 0.02 minutes (1 seconds)\n",
      "\n",
      "üíæ Features saved to: C:\\UCF_video_dataset\\TimeSformer_Features\n",
      "======================================================================\n",
      "\n",
      "üìÑ Metadata saved to: C:\\UCF_video_dataset\\TimeSformer_Features\\extraction_metadata.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cell 5: RUN OPTIMIZED FEATURE EXTRACTION\n",
    "Will skip already-processed videos automatically!\n",
    "\"\"\"\n",
    "\n",
    "# Run the OPTIMIZED extraction (with clip limiting)\n",
    "extraction_results = extract_features_optimized()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943764eb",
   "metadata": {},
   "source": [
    "## Cell 6: Verify Extracted Features\n",
    "\n",
    "Check that the feature bags are correctly shaped for MIL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "927cc4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FEATURE BAG VERIFICATION REPORT\n",
      "======================================================================\n",
      "\n",
      "üìÅ Checking features in: C:\\UCF_video_dataset\\TimeSformer_Features\n",
      "\n",
      "   Abuse               :   50 videos,   2989 total clips\n",
      "   Arrest              :   50 videos,   4613 total clips\n",
      "   Arson               :   50 videos,   4214 total clips\n",
      "   Assault             :   50 videos,   1995 total clips\n",
      "   Burglary            :  100 videos,   7290 total clips\n",
      "   Explosion           :   50 videos,   3910 total clips\n",
      "   Fighting            :   50 videos,   4013 total clips\n",
      "   Normal              :  950 videos, 111429 total clips\n",
      "   RoadAccidents       :  150 videos,   3973 total clips\n",
      "   Robbery             :  150 videos,   6505 total clips\n",
      "   Shooting            :   50 videos,   2270 total clips\n",
      "   Shoplifting         :   50 videos,   4491 total clips\n",
      "   Stealing            :  100 videos,   7236 total clips\n",
      "   Vandalism           :   50 videos,   2264 total clips\n",
      "\n",
      "üìä Summary:\n",
      "   Total Feature Files: 1900\n",
      "   Total Clips (Instances): 167192\n",
      "   Feature Dimension: 768\n",
      "\n",
      "üìê Shape Distribution (num_clips, 768):\n",
      "   (27, 768): 62 videos\n",
      "   (13, 768): 55 videos\n",
      "   (500, 768): 53 videos\n",
      "   (14, 768): 45 videos\n",
      "   (17, 768): 41 videos\n",
      "   (18, 768): 41 videos\n",
      "   (21, 768): 38 videos\n",
      "   (28, 768): 38 videos\n",
      "   (15, 768): 37 videos\n",
      "   (42, 768): 36 videos\n",
      "\n",
      "üìÑ Sample Feature Bag Analysis:\n",
      "   File: Abuse001_x264.npy\n",
      "   Shape: (42, 768)\n",
      "   Dtype: float32\n",
      "   Min: -3.8546\n",
      "   Max: 4.1871\n",
      "   Mean: -0.0163\n",
      "   Std: 0.9690\n",
      "\n",
      "======================================================================\n",
      "‚úÖ VERIFICATION COMPLETE\n",
      "   Each .npy file is a 'Bag of Instances' ready for MIL!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cell 6: Verify Extracted Features\n",
    "Check that Feature Bags are correctly shaped for MIL\n",
    "\"\"\"\n",
    "\n",
    "def verify_feature_bags(features_path=FEATURE_OUTPUT_DIR):\n",
    "    \"\"\"\n",
    "    Verify that all feature bags are correctly shaped.\n",
    "    Expected: Each .npy file should have shape (num_clips, 768)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FEATURE BAG VERIFICATION REPORT\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if not os.path.exists(features_path):\n",
    "        print(f\"‚ùå Features directory not found: {features_path}\")\n",
    "        return\n",
    "    \n",
    "    # Get all class folders\n",
    "    class_folders = [d for d in os.listdir(features_path) \n",
    "                     if os.path.isdir(os.path.join(features_path, d))]\n",
    "    \n",
    "    total_files = 0\n",
    "    total_clips = 0\n",
    "    shape_distribution = {}\n",
    "    \n",
    "    print(f\"\\nüìÅ Checking features in: {features_path}\\n\")\n",
    "    \n",
    "    for class_name in sorted(class_folders):\n",
    "        class_path = os.path.join(features_path, class_name)\n",
    "        npy_files = [f for f in os.listdir(class_path) if f.endswith('.npy')]\n",
    "        \n",
    "        class_clips = 0\n",
    "        for npy_file in npy_files:\n",
    "            file_path = os.path.join(class_path, npy_file)\n",
    "            features = np.load(file_path)\n",
    "            \n",
    "            # Track shape distribution\n",
    "            shape_key = f\"({features.shape[0]}, {features.shape[1]})\"\n",
    "            shape_distribution[shape_key] = shape_distribution.get(shape_key, 0) + 1\n",
    "            \n",
    "            class_clips += features.shape[0]\n",
    "            total_clips += features.shape[0]\n",
    "        \n",
    "        total_files += len(npy_files)\n",
    "        print(f\"   {class_name:20s}: {len(npy_files):4d} videos, {class_clips:6d} total clips\")\n",
    "    \n",
    "    print(f\"\\nüìä Summary:\")\n",
    "    print(f\"   Total Feature Files: {total_files}\")\n",
    "    print(f\"   Total Clips (Instances): {total_clips}\")\n",
    "    print(f\"   Feature Dimension: {FEATURE_DIM}\")\n",
    "    \n",
    "    print(f\"\\nüìê Shape Distribution (num_clips, 768):\")\n",
    "    for shape, count in sorted(shape_distribution.items(), key=lambda x: -x[1])[:10]:\n",
    "        print(f\"   {shape}: {count} videos\")\n",
    "    \n",
    "    # Sample one file to show detailed stats\n",
    "    print(f\"\\nüìÑ Sample Feature Bag Analysis:\")\n",
    "    sample_class = class_folders[0] if class_folders else None\n",
    "    if sample_class:\n",
    "        sample_path = os.path.join(features_path, sample_class)\n",
    "        sample_files = [f for f in os.listdir(sample_path) if f.endswith('.npy')]\n",
    "        if sample_files:\n",
    "            sample_file = os.path.join(sample_path, sample_files[0])\n",
    "            sample_features = np.load(sample_file)\n",
    "            \n",
    "            print(f\"   File: {sample_files[0]}\")\n",
    "            print(f\"   Shape: {sample_features.shape}\")\n",
    "            print(f\"   Dtype: {sample_features.dtype}\")\n",
    "            print(f\"   Min: {sample_features.min():.4f}\")\n",
    "            print(f\"   Max: {sample_features.max():.4f}\")\n",
    "            print(f\"   Mean: {sample_features.mean():.4f}\")\n",
    "            print(f\"   Std: {sample_features.std():.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ VERIFICATION COMPLETE\")\n",
    "    print(\"   Each .npy file is a 'Bag of Instances' ready for MIL!\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "\n",
    "# Run verification\n",
    "verify_feature_bags()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4868344",
   "metadata": {},
   "source": [
    "## Cell 7: Summary and Next Steps\n",
    "\n",
    "Final summary and preparation for Phase 3 (MIL Training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "348ade99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PHASE 2 COMPLETE: FEATURE EXTRACTION (MIL-READY)\n",
      "======================================================================\n",
      "\n",
      "‚úÖ What was accomplished:\n",
      "   1. Loaded pretrained TimeSformer model (facebook/timesformer-base-finetuned-k400)\n",
      "   2. Processed each video's \"Bag of Clips\" from Phase 1\n",
      "   3. Extracted 768-dimensional [CLS] token features for EACH CLIP\n",
      "   4. Saved Feature Bags as .npy files\n",
      "\n",
      "üìÅ Output Structure:\n",
      "   TimeSformer_Features/\n",
      "   ‚îú‚îÄ‚îÄ Abuse/\n",
      "   ‚îÇ   ‚îú‚îÄ‚îÄ Abuse001.npy      ‚Üí Shape: (num_clips, 768) e.g. (87, 768)\n",
      "   ‚îÇ   ‚îî‚îÄ‚îÄ Abuse002.npy      ‚Üí Shape: (num_clips, 768) e.g. (124, 768)\n",
      "   ‚îú‚îÄ‚îÄ Explosion/\n",
      "   ‚îÇ   ‚îî‚îÄ‚îÄ Explosion001.npy  ‚Üí Shape: (num_clips, 768) e.g. (156, 768)\n",
      "   ‚îú‚îÄ‚îÄ Normal/\n",
      "   ‚îÇ   ‚îî‚îÄ‚îÄ Normal001.npy     ‚Üí Shape: (num_clips, 768) e.g. (203, 768)\n",
      "   ‚îî‚îÄ‚îÄ extraction_metadata.json\n",
      "\n",
      "üéØ Why This Shape Matters for MIL:\n",
      "   ‚Ä¢ Each video is now a \"BAG\" of multiple instances (clips)\n",
      "   ‚Ä¢ MIL can compare instances within and across bags\n",
      "   ‚Ä¢ Example: \"Clips 1-40 look normal, but Clip 45 is anomalous\"\n",
      "   ‚Ä¢ This enables FRAME-LEVEL LOCALIZATION (key thesis requirement!)\n",
      "\n",
      "üöÄ Next Steps (Phase 3 - MIL Training):\n",
      "   1. Load Feature Bags\n",
      "   2. Implement MIL Network with Attention Mechanism\n",
      "   3. Train with:\n",
      "      - Ranking Loss: max(anomaly_scores) > max(normal_scores)\n",
      "      - Focal Loss: Handle class imbalance\n",
      "      - Temporal Smoothness: Consistent adjacent predictions\n",
      "   4. Evaluate: AUC-ROC, Per-Frame Localization\n",
      "\n",
      "======================================================================\n",
      "\n",
      "üßπ GPU memory cleared. Current usage: 0.45 GB\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cell 7: Phase 2 Summary\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PHASE 2 COMPLETE: FEATURE EXTRACTION (MIL-READY)\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "‚úÖ What was accomplished:\n",
    "   1. Loaded pretrained TimeSformer model (facebook/timesformer-base-finetuned-k400)\n",
    "   2. Processed each video's \"Bag of Clips\" from Phase 1\n",
    "   3. Extracted 768-dimensional [CLS] token features for EACH CLIP\n",
    "   4. Saved Feature Bags as .npy files\n",
    "\n",
    "üìÅ Output Structure:\n",
    "   TimeSformer_Features/\n",
    "   ‚îú‚îÄ‚îÄ Abuse/\n",
    "   ‚îÇ   ‚îú‚îÄ‚îÄ Abuse001.npy      ‚Üí Shape: (num_clips, 768) e.g. (87, 768)\n",
    "   ‚îÇ   ‚îî‚îÄ‚îÄ Abuse002.npy      ‚Üí Shape: (num_clips, 768) e.g. (124, 768)\n",
    "   ‚îú‚îÄ‚îÄ Explosion/\n",
    "   ‚îÇ   ‚îî‚îÄ‚îÄ Explosion001.npy  ‚Üí Shape: (num_clips, 768) e.g. (156, 768)\n",
    "   ‚îú‚îÄ‚îÄ Normal/\n",
    "   ‚îÇ   ‚îî‚îÄ‚îÄ Normal001.npy     ‚Üí Shape: (num_clips, 768) e.g. (203, 768)\n",
    "   ‚îî‚îÄ‚îÄ extraction_metadata.json\n",
    "\n",
    "üéØ Why This Shape Matters for MIL:\n",
    "   ‚Ä¢ Each video is now a \"BAG\" of multiple instances (clips)\n",
    "   ‚Ä¢ MIL can compare instances within and across bags\n",
    "   ‚Ä¢ Example: \"Clips 1-40 look normal, but Clip 45 is anomalous\"\n",
    "   ‚Ä¢ This enables FRAME-LEVEL LOCALIZATION (key thesis requirement!)\n",
    "\n",
    "üöÄ Next Steps (Phase 3 - MIL Training):\n",
    "   1. Load Feature Bags\n",
    "   2. Implement MIL Network with Attention Mechanism\n",
    "   3. Train with:\n",
    "      - Ranking Loss: max(anomaly_scores) > max(normal_scores)\n",
    "      - Focal Loss: Handle class imbalance\n",
    "      - Temporal Smoothness: Consistent adjacent predictions\n",
    "   4. Evaluate: AUC-ROC, Per-Frame Localization\n",
    "\"\"\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Clear GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    allocated = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "    print(f\"\\nüßπ GPU memory cleared. Current usage: {allocated:.2f} GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
